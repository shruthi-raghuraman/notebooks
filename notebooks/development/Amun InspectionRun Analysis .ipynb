{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Retrieve-inspection-jobs-from-Ceph\" data-toc-modified-id=\"Retrieve-inspection-jobs-from-Ceph-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Retrieve inspection jobs from Ceph</a></span></li><li><span><a href=\"#Describe-the-structure-of-an-inspection-job-log\" data-toc-modified-id=\"Describe-the-structure-of-an-inspection-job-log-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Describe the structure of an inspection job log</a></span></li><li><span><a href=\"#Mapping-InspectionRun-JSON-to-pandas-DataFrame\" data-toc-modified-id=\"Mapping-InspectionRun-JSON-to-pandas-DataFrame-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Mapping InspectionRun JSON to pandas DataFrame</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-importance-analysis\" data-toc-modified-id=\"Feature-importance-analysis-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Feature importance analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Status\" data-toc-modified-id=\"Status-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Status</a></span></li><li><span><a href=\"#Specification\" data-toc-modified-id=\"Specification-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Specification</a></span></li><li><span><a href=\"#Job-log\" data-toc-modified-id=\"Job-log-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Job log</a></span></li></ul></li></ul></li><li><span><a href=\"#Profile-InspectionRun-duration\" data-toc-modified-id=\"Profile-InspectionRun-duration-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Profile InspectionRun duration</a></span></li><li><span><a href=\"#Plot-InspectionRun-duration\" data-toc-modified-id=\"Plot-InspectionRun-duration-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Plot InspectionRun duration</a></span></li><li><span><a href=\"#Library-usage\" data-toc-modified-id=\"Library-usage-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Library usage</a></span></li><li><span><a href=\"#Grouping-and-filtering\" data-toc-modified-id=\"Grouping-and-filtering-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Grouping and filtering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Grouping-based-on-hardware-platform\" data-toc-modified-id=\"Grouping-based-on-hardware-platform-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Grouping based on hardware platform</a></span></li><li><span><a href=\"#Grouping-based-on-exit-status\" data-toc-modified-id=\"Grouping-based-on-exit-status-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Grouping based on exit status</a></span></li><li><span><a href=\"#Creation-of-duration-dataframe-from-filtered-inspection-results\" data-toc-modified-id=\"Creation-of-duration-dataframe-from-filtered-inspection-results-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Creation of duration dataframe from filtered inspection results</a></span></li></ul></li><li><span><a href=\"#Visualizing-grouped-data\" data-toc-modified-id=\"Visualizing-grouped-data-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Visualizing grouped data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Grouping-based-on-software-stack\" data-toc-modified-id=\"Grouping-based-on-software-stack-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Grouping based on software stack</a></span></li><li><span><a href=\"#Grouping-based-on-OS-system\" data-toc-modified-id=\"Grouping-based-on-OS-system-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Grouping based on OS system</a></span></li></ul></li><li><span><a href=\"#Further-analysis\" data-toc-modified-id=\"Further-analysis-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Further analysis</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Amun InspectionRun Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "The notebook has been created to have the possibility to analyze statistics regarding the inspections results produced by Amun (https://github.com/thoth-station/amun-api). In particular the statistical analysis of inspection job logs will provide a measure of the quality of the inspection test application. The conclusions are important in order to identify the best environment to perform tests for AI stacks.\n",
    "\n",
    "Input of the analysis are ~300 inspection job logs. For this initial analysis we considered:\n",
    "- same software stack\n",
    "- same OS\n",
    "- same performance test\n",
    "\n",
    "This notebook can be reused for analysis of inspection job logs and can be improved. The structure of this notebook is made by several main parts:\n",
    "- Description of an inspection job log;\n",
    "- Profiling of all inspection job logs and feature analysis\n",
    "- Creation of library for grouping and filtering inspection jobs\n",
    "- Visualizing single results and results separated by category (software stacks, OS, hw, architecture, ..)\n",
    "- Analyze the results\n",
    "- Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T12:59:30.825932Z",
     "start_time": "2019-06-07T12:59:30.790067Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import functools\n",
    "import re\n",
    "\n",
    "import textwrap\n",
    "import typing\n",
    "\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "from typing import Callable, Iterable\n",
    "\n",
    "from collections import namedtuple\n",
    "from prettyprinter import pformat\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T12:59:32.360673Z",
     "start_time": "2019-06-07T12:59:30.830682Z"
    },
    "init_cell": true,
    "require": [
     "notebook/js/codecell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pandas_profiling import ProfileReport as profile\n",
    "from pandas.io.json import json_normalize\n",
    "from thoth.storages import InspectionResultsStore\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 800)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T12:59:33.298333Z",
     "start_time": "2019-06-07T12:59:32.362869Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "import plotly\n",
    "import plotly.offline as py\n",
    "\n",
    "from plotly import graph_objs as go\n",
    "from plotly import figure_factory as ff\n",
    "from plotly import tools\n",
    "\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "\n",
    "# plotly\n",
    "init_notebook_mode()\n",
    "\n",
    "# cufflinks\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve inspection jobs from Ceph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T12:59:33.307471Z",
     "start_time": "2019-06-07T12:59:33.300257Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%env THOTH_DEPLOYMENT_NAME     thoth-core-upshift-stage\n",
    "%env THOTH_CEPH_BUCKET         thoth\n",
    "%env THOTH_CEPH_BUCKET_PREFIX  data/thoth\n",
    "%env THOTH_S3_ENDPOINT_URL     https://s3.upshift.redhat.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T12:59:33.374003Z",
     "start_time": "2019-06-07T12:59:33.309380Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "inspection_store = InspectionResultsStore(region=\"eu-central-1\")\n",
    "inspection_store.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Describe the structure of an inspection job log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:05:49.469754Z",
     "start_time": "2019-06-06T14:05:49.458333Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_structure_json(input_json, upper_key: str, level: int, json_structure):\n",
    "    \"\"\"Convert a json file structure into a list with rows showing tree depths, keys and values\"\"\"\n",
    "    level += 1\n",
    "    for key in input_json.keys():\n",
    "        if type(input_json[key]) is dict:\n",
    "            json_structure.append(\n",
    "                [level, upper_key, key, [k for k in input_json[key].keys()]]\n",
    "            )\n",
    "\n",
    "            extract_structure_json(\n",
    "                input_json[key], f\"{upper_key}__{key}\", level, json_structure\n",
    "            )\n",
    "        else:\n",
    "            json_structure.append([level, upper_key, key, input_json[key]])\n",
    "\n",
    "    return json_structure\n",
    "\n",
    "\n",
    "def filter_dfs(df_s, filter_df):\n",
    "    \"\"\"Filter the specific dataframe created for a certain key, combination of keys or for a tree depth\"\"\"\n",
    "    if type(filter_df) is str:\n",
    "        available_keys = set(df_s[\"Current_key\"].values)\n",
    "        available_combined_keys = set(df_s[\"Upper_keys\"].values)\n",
    "\n",
    "        if filter_df in available_keys:\n",
    "            ndf = df_s[df_s[\"Current_key\"].str.contains(f\"^{filter_df}$\", regex=True)]\n",
    "\n",
    "        elif filter_df in available_combined_keys:\n",
    "            ndf = df_s[df_s[\"Upper_keys\"].str.contains(f\"{filter_df}$\", regex=True)]\n",
    "        else:\n",
    "            print(\"The key is not in the json\")\n",
    "            ndf = \"\".join(\n",
    "                [\n",
    "                    f\"The available keys are (WARNING: Some of the keys have no leafs):{available_keys} \",\n",
    "                    f\"The available combined keys are: {available_combined_keys}\",\n",
    "                ]\n",
    "            )\n",
    "    elif type(filter_df) is int:\n",
    "        max_depth = df_s[\"Tree_depth\"].max()\n",
    "        if filter_df <= max_depth:\n",
    "            ndf = df_s[df_s[\"Tree_depth\"] == filter_df]\n",
    "        else:\n",
    "            ndf = f\"The maximum tree depth available is: {max_depth}\"\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can take a look at the inspection job structure from the point of view of the tree depth, considering a key or a combination of keys in order to understand the common inputs for all inspections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:05:57.734633Z",
     "start_time": "2019-06-06T14:05:51.400532Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_id, doc = next(inspection_store.iterate_results())\n",
    "df_structure = pd.DataFrame(extract_structure_json(doc, \"\", 0, []))\n",
    "df_structure.columns = [\"Tree_depth\", \"Upper_keys\", \"Current_key\", \"Value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check the structure of an inspection job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:05:58.381766Z",
     "start_time": "2019-06-06T14:05:58.312822Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:23:05.948842Z",
     "start_time": "2019-05-17T11:23:05.944961Z"
    },
    "hidden": true
   },
   "source": [
    "Check memory requested for build and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:05:59.500443Z",
     "start_time": "2019-06-06T14:05:59.488228Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filter_dfs(df_structure, \"memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check hardware requested for build and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:06:00.399938Z",
     "start_time": "2019-06-06T14:06:00.389237Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filter_dfs(df_structure, \"__specification__build__requests__hardware\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:06:00.804480Z",
     "start_time": "2019-06-06T14:06:00.791952Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filter_dfs(df_structure, \"__specification__run__requests__hardware\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Verify hardware info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:06:01.452770Z",
     "start_time": "2019-06-06T14:06:01.436002Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filter_dfs(df_structure, \"__job_log__hwinfo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check CPU information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:06:01.887479Z",
     "start_time": "2019-06-06T14:06:01.866785Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter_dfs(df_structure, \"__job_log__hwinfo__cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check Platform information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:06:02.284660Z",
     "start_time": "2019-06-06T14:06:02.273374Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filter_dfs(df_structure, \"__job_log__hwinfo__platform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check source of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:06:02.652063Z",
     "start_time": "2019-06-06T14:06:02.638292Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filter_dfs(df_structure, \"__specification__python__requirements_locked___meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check which libraries are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:06:03.011948Z",
     "start_time": "2019-06-06T14:06:02.980024Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for package in filter_dfs(\n",
    "    df_structure, \"__specification__python__requirements_locked__default\"\n",
    ")[\"Current_key\"].values:\n",
    "    dfp = filter_dfs(\n",
    "        df_structure,\n",
    "        f\"__specification__python__requirements_locked__default__{package}\",\n",
    "    )\n",
    "    print(\n",
    "        \"{:15}  {}\".format(\n",
    "            package, dfp[dfp[\"Current_key\"].str.contains(\"version\")][\"Value\"].values[0]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check OS used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:06:04.323236Z",
     "start_time": "2019-06-06T14:06:04.309559Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filter_dfs(df_structure, \"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check the micro-benchmark used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:06:05.688806Z",
     "start_time": "2019-06-06T14:06:05.677462Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filter_dfs(df_structure, \"script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:06:06.372845Z",
     "start_time": "2019-06-06T14:06:06.350002Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filter_dfs(df_structure, \"script_sha256\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "lines_to_next_cell": 0
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Mapping InspectionRun JSON to pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:07:12.627615Z",
     "start_time": "2019-06-07T12:59:33.376223Z"
    },
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "inspection_results = []\n",
    "\n",
    "for document_id, document in inspection_store.iterate_results():\n",
    "    # pop build logs to save some memory (not necessary for now)\n",
    "    document[\"build_log\"] = None\n",
    "\n",
    "    inspection_results.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:07:15.739738Z",
     "start_time": "2019-06-07T13:07:12.629234Z"
    },
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "df = json_normalize(inspection_results, sep = \".\")  # each row resembles InspectionResult\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Feature importance analysis\n",
    "\n",
    "For the purposes of the performance analysis we take under consideration the impact of a variable on the performance score, the variance of the features is therefore an important indicator. We can assume that the more variance feature evinces, the higher is its impact on the performance measure stability.\n",
    "\n",
    "We can perform profiling as the first stage of this analysis to identify constants which won't affect the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:07:15.756692Z",
     "start_time": "2019-06-07T13:07:15.749392Z"
    },
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "f\"The original DataFrame contains  {len(inspection_results)}  columns\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These are the top-level keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:07:15.767866Z",
     "start_time": "2019-06-07T13:07:15.759368Z"
    },
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "inspection_results[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:14.481781Z",
     "start_time": "2019-05-17T11:57:14.390670Z"
    },
    "hidden": true,
    "require": [
     "base/js/events",
     "datatables.net",
     "d3",
     "jupyter-datatables"
    ]
   },
   "outputs": [],
   "source": [
    "df_status = df.filter(regex=\"status\")\n",
    "\n",
    "date_columns = df_status.filter(regex=\"started_at|finished_at\").columns\n",
    "for col in date_columns:\n",
    "    df_status[col] = df[col].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:15.618682Z",
     "start_time": "2019-05-17T11:57:14.579930Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p = profile(df_status)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "According to the profiling, we can drop the values with the constant value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:15.637758Z",
     "start_time": "2019-05-17T11:57:15.619957Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rejected = p.description_set[\"variables\"].query(\n",
    "    \"distinct_count <= 1 & type != 'UNSUPPORTED'\"\n",
    ")\n",
    "rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:15.643079Z",
     "start_time": "2019-05-17T11:57:15.638981Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop(rejected.index, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:15.906621Z",
     "start_time": "2019-05-17T11:57:15.899603Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_spec = df.filter(regex=\"specification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:17.520060Z",
     "start_time": "2019-05-17T11:57:16.541214Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p = profile(df_spec)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:17.563644Z",
     "start_time": "2019-05-17T11:57:17.522102Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rejected = p.description_set[\"variables\"].query(\n",
    "    \"distinct_count <= 1 & type != 'UNSUPPORTED'\"\n",
    ")\n",
    "rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "exclude versions, we might wanna use them later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:17.587560Z",
     "start_time": "2019-05-17T11:57:17.565183Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rejected = rejected.filter(regex=\"^((?!version).)*$\", axis=0)\n",
    "rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:17.877810Z",
     "start_time": "2019-05-17T11:57:17.873489Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop(rejected.index, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Job log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:19.177784Z",
     "start_time": "2019-05-17T11:57:19.164907Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_job = df.filter(regex=\"job_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:22.084871Z",
     "start_time": "2019-05-17T11:57:19.368302Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p = profile(df_job)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:22.136107Z",
     "start_time": "2019-05-17T11:57:22.086412Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rejected = p.description_set[\"variables\"].query(\n",
    "    \"distinct_count <= 1 & type != 'UNSUPPORTED'\"\n",
    ")\n",
    "rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:22.142238Z",
     "start_time": "2019-05-17T11:57:22.137661Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop(rejected.index, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:07:15.784555Z",
     "start_time": "2019-06-07T13:07:15.770212Z"
    },
    "code_folding": [
     6
    ],
    "hidden": true,
    "init_cell": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def process_inspection_results(\n",
    "    inspection_results: List[dict],\n",
    "    exclude: Union[list, set] = None,\n",
    "    apply: List[Tuple] = None,\n",
    "    drop: bool = True,\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Process inspection result into pd.DataFrame.\"\"\"\n",
    "    if not inspection_results:\n",
    "        return ValueError(\"Empty iterable provided.\")\n",
    "\n",
    "    exclude = exclude or []\n",
    "    apply = apply or ()\n",
    "\n",
    "    df = json_normalize(\n",
    "        inspection_results, sep=\"__\"\n",
    "    )  # each row resembles InspectionResult\n",
    "\n",
    "    if len(df) <= 1:\n",
    "        return df\n",
    "\n",
    "    for regex, func in apply:\n",
    "        for col in df.filter(regex=regex).columns:\n",
    "            df[col] = df[col].apply(func)\n",
    "\n",
    "    keys = [k for k in inspection_results[0] if not k in exclude]\n",
    "    for k in keys:\n",
    "        if k in exclude:\n",
    "            continue\n",
    "        d = df.filter(regex=k)\n",
    "        p = profile(d)\n",
    "\n",
    "        rejected = (\n",
    "            p.description_set[\"variables\"]\n",
    "            .query(\"distinct_count <= 1 & type != 'UNSUPPORTED'\")\n",
    "            .filter(regex=\"^((?!version).)*$\", axis=0)\n",
    "        )  # explicitly include versions\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Rejected columns: \", rejected.index)\n",
    "\n",
    "        if drop:\n",
    "            df.drop(rejected.index, axis=1, inplace=True)\n",
    "\n",
    "    df = (\n",
    "        df.eval(\n",
    "            \"status__job__duration   = status__job__finished_at   - status__job__started_at\",\n",
    "            engine=\"python\"\n",
    "        )\n",
    "        .eval(\n",
    "            \"status__build__duration = status__build__finished_at - status__build__started_at\",\n",
    "            engine=\"python\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:15:32.071212Z",
     "start_time": "2019-06-06T14:15:25.163045Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = process_inspection_results(\n",
    "    inspection_results,\n",
    "    exclude=[\"build_log\", \"created\", \"inspection_id\"],\n",
    "    apply=[(\"created|started_at|finished_at\", pd.to_datetime)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "lines_to_next_cell": 0
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Profile InspectionRun duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T14:59:04.447953Z",
     "start_time": "2019-06-07T14:59:04.420280Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_duration = (\n",
    "    df.filter(like=\"duration\")\n",
    "    .rename(columns=lambda s: s.replace(\"status__\", \"\").replace(\"__\", \"_\"))\n",
    "    .apply(lambda ts: pd.to_timedelta(ts).dt.total_seconds())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T14:58:38.120678Z",
     "start_time": "2019-06-07T14:58:37.092292Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p = profile(df_duration)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T14:58:38.162503Z",
     "start_time": "2019-06-07T14:58:38.129048Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stats = p.description_set[\"variables\"].drop([\"histogram\", \"mini_histogram\"], axis=1)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Plot InspectionRun duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Make sure that the versions are constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T14:58:46.926470Z",
     "start_time": "2019-06-07T14:58:46.865005Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.filter(regex=\"python.*version\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Visualize statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T14:58:47.254781Z",
     "start_time": "2019-06-07T14:58:47.177659Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = df_duration.iplot(\n",
    "    kind=\"box\", title=\"InspectionRun duration\", yTitle=\"duration [s]\", asFigure=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T14:58:47.422260Z",
     "start_time": "2019-06-07T14:58:47.258018Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_duration.iplot(\n",
    "    fill=\"tonexty\",\n",
    "    kind=\"scatter\",\n",
    "    title=\"InspectionRun duration\",\n",
    "    yTitle=\"duration [s]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T14:58:47.573143Z",
     "start_time": "2019-06-07T14:58:47.424103Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_duration = (\n",
    "    df_duration.eval(\n",
    "        \"job_duration_mean           = job_duration.mean()\", engine=\"python\"\n",
    "    )\n",
    "    .eval(\"build_duration_mean         = build_duration.mean()\", engine=\"python\")\n",
    "    .eval(\n",
    "        \"job_duration_upper_bound    = job_duration + job_duration.std()\",\n",
    "        engine=\"python\",\n",
    "    )\n",
    "    .eval(\n",
    "        \"job_duration_lower_bound    = job_duration - job_duration.std()\",\n",
    "        engine=\"python\",\n",
    "    )\n",
    "    .eval(\n",
    "        \"build_duration_upper_bound  = build_duration + build_duration.std()\",\n",
    "        engine=\"python\",\n",
    "    )\n",
    "    .eval(\n",
    "        \"build_duration_lower_bound  = build_duration - build_duration.std()\",\n",
    "        engine=\"python\",\n",
    "    )\n",
    ")\n",
    "\n",
    "upper_bound = go.Scatter(\n",
    "    name=\"Upper Bound\",\n",
    "    x=df_duration.index,\n",
    "    y=df_duration.job_duration_upper_bound,\n",
    "    mode=\"lines\",\n",
    "    marker=dict(color=\"lightgray\"),\n",
    "    line=dict(width=0),\n",
    "    fillcolor=\"rgba(68, 68, 68, 0.3)\",\n",
    "    fill=\"tonexty\",\n",
    ")\n",
    "\n",
    "trace = go.Scatter(\n",
    "    name=\"Duration\",\n",
    "    x=df_duration.index,\n",
    "    y=df_duration.job_duration,\n",
    "    mode=\"lines\",\n",
    "    line=dict(color=\"rgb(31, 119, 180)\"),\n",
    "    fillcolor=\"rgba(68, 68, 68, 0.3)\",\n",
    "    fill=\"tonexty\",\n",
    ")\n",
    "\n",
    "lower_bound = go.Scatter(\n",
    "    name=\"Lower Bound\",\n",
    "    x=df_duration.index,\n",
    "    y=df_duration.job_duration_lower_bound,\n",
    "    marker=dict(color=\"lightgray\"),\n",
    "    line=dict(width=0),\n",
    "    mode=\"lines\",\n",
    ")\n",
    "\n",
    "data = [lower_bound, trace, upper_bound]\n",
    "\n",
    "m = stats.loc[\"job_duration\"][\"mean\"]\n",
    "layout = go.Layout(\n",
    "    yaxis=dict(title=\"duration [s]\"),\n",
    "    shapes=[\n",
    "        {\n",
    "            \"type\": \"line\",\n",
    "            \"x0\": 0,\n",
    "            \"x1\": len(df_duration.index),\n",
    "            \"y0\": m,\n",
    "            \"y1\": m,\n",
    "            \"line\": {\"color\": \"red\", \"dash\": \"longdash\"},\n",
    "        }\n",
    "    ],\n",
    "    title=\"InspectionRun job duration\",\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig, filename=\"pandas-time-series-error-bars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T11:57:29.180841Z",
     "start_time": "2019-05-17T11:57:29.099723Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins = np.lib.histograms._hist_bin_auto(df_duration.job_duration.values, None)\n",
    "\n",
    "df_duration.job_duration.iplot(\n",
    "    title=\"InspectionRun job distribution\",\n",
    "    xTitle=\"duration [s]\",\n",
    "    yTitle=\"count\",\n",
    "    kind=\"hist\",\n",
    "    bins=int(np.ceil(bins)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "lines_to_next_cell": 0
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Library usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T19:28:25.038396Z",
     "start_time": "2019-06-07T19:28:25.005495Z"
    },
    "code_folding": [
     8,
     65,
     81,
     100,
     175
    ],
    "init_cell": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"Thoth InspectionRun dashboard app.\"\"\"\n",
    "\n",
    "pd.set_option(\"precision\", 4)\n",
    "pd.set_option(\"colheader_justify\", \"center\")\n",
    "\n",
    "\n",
    "def create_duration_dataframe(inspection_df: pd.DataFrame):\n",
    "    \"\"\"Compute statistics and duration DataFrame.\"\"\"\n",
    "    if len(inspection_df) <= 0:\n",
    "        raise ValueError(\"Empty DataFrame provided\")\n",
    "\n",
    "    try:\n",
    "        inspection_df.drop(\"build_log\", axis=1, inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    data = (\n",
    "        inspection_df.filter(like=\"duration\")\n",
    "        .rename(columns=lambda s: s.replace(\"status__\", \"\").replace(\"__\", \"_\"))\n",
    "        .apply(lambda ts: pd.to_timedelta(ts).dt.total_seconds())\n",
    "    )\n",
    "\n",
    "    def compute_duration_stats(group):\n",
    "        return (\n",
    "            group.eval(\n",
    "                \"job_duration_mean           = job_duration.mean()\", engine=\"python\"\n",
    "            )\n",
    "            .eval(\n",
    "                \"job_duration_upper_bound    = job_duration + job_duration.std()\",\n",
    "                engine=\"python\",\n",
    "            )\n",
    "            .eval(\n",
    "                \"job_duration_lower_bound    = job_duration - job_duration.std()\",\n",
    "                engine=\"python\",\n",
    "            )\n",
    "            .eval(\n",
    "                \"build_duration_mean         = build_duration.mean()\", engine=\"python\"\n",
    "            )\n",
    "            .eval(\n",
    "                \"build_duration_upper_bound  = build_duration + build_duration.std()\",\n",
    "                engine=\"python\",\n",
    "            )\n",
    "            .eval(\n",
    "                \"build_duration_lower_bound  = build_duration - build_duration.std()\",\n",
    "                engine=\"python\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if isinstance(inspection_df.index, pd.MultiIndex):\n",
    "        n_levels = len(inspection_df.index.levels)\n",
    "\n",
    "        # compute duration stats for each group separately\n",
    "        data = data.groupby(level=list(range(n_levels - 1)), sort=False).apply(\n",
    "            compute_duration_stats\n",
    "        )\n",
    "    else:\n",
    "        data = compute_duration_stats(data)\n",
    "\n",
    "    return data.round(4)\n",
    "\n",
    "\n",
    "def create_duration_box(\n",
    "    data: pd.DataFrame, columns: Union[str, List[str]] = None, **kwargs\n",
    "):\n",
    "    \"\"\"Create duration Box plot.\"\"\"\n",
    "    columns = columns if columns is not None else data.filter(regex=\"duration$\").columns\n",
    "\n",
    "    figure = data[columns].iplot(\n",
    "        kind=\"box\",\n",
    "        title=kwargs.pop(\"title\", \"InspectionRun duration\"),\n",
    "        yTitle=\"duration [s]\",\n",
    "        asFigure=True,\n",
    "    )\n",
    "\n",
    "    return figure\n",
    "\n",
    "\n",
    "def create_duration_scatter(\n",
    "    data: pd.DataFrame, columns: Union[str, List[str]] = None, **kwargs\n",
    "):\n",
    "    columns = columns if columns is not None else data.filter(regex=\"duration$\").columns\n",
    "\n",
    "    figure = data[columns].iplot(\n",
    "        kind=\"scatter\",\n",
    "        title=kwargs.pop(\"title\", \"InspectionRun duration\"),\n",
    "        yTitle=\"duration [s]\",\n",
    "        xTitle=\"inspection ID\",\n",
    "        asFigure=True,\n",
    "    )\n",
    "\n",
    "    return figure\n",
    "\n",
    "\n",
    "def create_duration_scatter_with_bounds(\n",
    "    data: pd.DataFrame,\n",
    "    col: str,\n",
    "    index: Union[list, pd.Index, pd.RangeIndex] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Create duration Scatter plot.\"\"\"\n",
    "    df_duration = (\n",
    "        data[[col]]\n",
    "        .eval(f\"upper_bound = {col} + {col}.std()\", engine=\"python\")\n",
    "        .eval(f\"lower_bound = {col} - {col}.std()\", engine=\"python\")\n",
    "    )\n",
    "\n",
    "    index = index if index is not None else df_duration.index\n",
    "\n",
    "    if isinstance(index, pd.MultiIndex):\n",
    "        index = (\n",
    "            index.levels[-1] if len(index.levels[-1]) == len(df) else np.arange(len(df))\n",
    "        )\n",
    "\n",
    "    upper_bound = go.Scatter(\n",
    "        name=\"Upper Bound\",\n",
    "        x=index,\n",
    "        y=df_duration.upper_bound,\n",
    "        mode=\"lines\",\n",
    "        marker=dict(color=\"lightgray\"),\n",
    "        line=dict(width=0),\n",
    "        fillcolor=\"rgba(68, 68, 68, 0.3)\",\n",
    "        fill=\"tonexty\",\n",
    "    )\n",
    "\n",
    "    trace = go.Scatter(\n",
    "        name=\"Duration\",\n",
    "        x=index,\n",
    "        y=df_duration[col],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"rgb(31, 119, 180)\"),\n",
    "        fillcolor=\"rgba(68, 68, 68, 0.3)\",\n",
    "        fill=\"tonexty\",\n",
    "    )\n",
    "\n",
    "    lower_bound = go.Scatter(\n",
    "        name=\"Lower Bound\",\n",
    "        x=index,\n",
    "        y=df_duration.lower_bound,\n",
    "        marker=dict(color=\"lightgray\"),\n",
    "        line=dict(width=0),\n",
    "        mode=\"lines\",\n",
    "    )\n",
    "\n",
    "    data = [lower_bound, trace, upper_bound]\n",
    "    m = df_duration[col].mean()\n",
    "\n",
    "    layout = go.Layout(\n",
    "        yaxis=dict(title=\"duration [s]\"),\n",
    "        xaxis=dict(title=\"inspection ID\"),\n",
    "        shapes=[\n",
    "            {\n",
    "                \"type\": \"line\",\n",
    "                \"x0\": 0,\n",
    "                \"x1\": len(index),\n",
    "                \"y0\": m,\n",
    "                \"y1\": m,\n",
    "                \"line\": {\"color\": \"red\", \"dash\": \"longdash\"},\n",
    "            }\n",
    "        ],\n",
    "        title=kwargs.pop(\"title\", \"InspectionRun duration\"),\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_duration_histogram(\n",
    "    data: pd.DataFrame,\n",
    "    columns: Union[str, List[str]] = None,\n",
    "    bins: int = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Create duration histogram.\"\"\"\n",
    "    columns = columns if columns is not None else data.filter(regex=\"duration$\").columns\n",
    "\n",
    "    if not bins:\n",
    "        bins = np.max(\n",
    "            [\n",
    "                np.lib.histograms._hist_bin_auto(data[col].values, None)\n",
    "                for col in columns\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    figure = data[columns].iplot(\n",
    "        title=kwargs.pop(\"title\", \"InspectionRun distribution\"),\n",
    "        yTitle=\"count\",\n",
    "        xTitle=\"durations [ms]\",\n",
    "        kind=\"hist\",\n",
    "        bins=int(np.ceil(bins)),\n",
    "        asFigure=True,\n",
    "    )\n",
    "\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T12:36:18.138234Z",
     "start_time": "2019-06-03T12:36:07.268166Z"
    }
   },
   "outputs": [],
   "source": [
    "df = process_inspection_results(\n",
    "    inspection_results,\n",
    "    exclude=[\"build_log\", \"created\", \"inspection_id\"],\n",
    "    apply=[(\"created|started_at|finished_at\", pd.to_datetime)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:16:12.434347Z",
     "start_time": "2019-06-06T14:16:12.411606Z"
    }
   },
   "outputs": [],
   "source": [
    "df_duration = create_duration_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:16:13.945207Z",
     "start_time": "2019-06-06T14:16:13.859286Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = create_duration_box(df_duration, [\"build_duration\", \"job_duration\"])\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:16:15.385540Z",
     "start_time": "2019-06-06T14:16:15.319904Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = create_duration_scatter(\n",
    "    df_duration, \"job_duration\", title=\"InspectionRun job duration\"\n",
    ")\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:16:16.468684Z",
     "start_time": "2019-06-06T14:16:16.400214Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = create_duration_scatter(\n",
    "    df_duration, \"build_duration\", title=\"InspectionRun build duration\"\n",
    ")\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-06T14:16:17.244200Z",
     "start_time": "2019-06-06T14:16:17.184171Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = create_duration_histogram(df_duration, [\"job_duration\"])\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and filtering\n",
    "\n",
    "The goal of this part is to have a function which divides inspection jobs into “categories”, the function accepts loaded inspection JSON files and a key which should be used to split input inspection documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:07:15.835755Z",
     "start_time": "2019-06-07T13:07:15.815363Z"
    },
    "code_folding": [
     6
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def _resolve_query(\n",
    "    query: str,\n",
    "    context: pd.DataFrame = None,\n",
    "    resolvers: tuple = None,\n",
    "    engine: str = None,\n",
    "    parser: str = \"pandas\",\n",
    "):\n",
    "    \"\"\"Resolve query in the given context.\"\"\"\n",
    "    from pandas.core.computation.expr import Expr\n",
    "    from pandas.core.computation.eval import _ensure_scope\n",
    "\n",
    "    if not query:\n",
    "        return context\n",
    "\n",
    "    q = query\n",
    "    q = re.sub(r\"\\[\\(\", \"\", q)\n",
    "    q = re.sub(r\"\\b(\\d)+\\b\", \"\", q)\n",
    "    q = re.sub(r\"[+\\-\\*:!<>=~.|&%]\", \" \", q)\n",
    "\n",
    "    # get our (possibly passed-in) scope\n",
    "    resolvers = resolvers or ()\n",
    "    if isinstance(context, pd.DataFrame):\n",
    "        index_resolvers = context._get_index_resolvers()\n",
    "        resolvers = tuple(resolvers) + (dict(context.iteritems()), index_resolvers)\n",
    "\n",
    "    repl = []\n",
    "    for idx, resolver in enumerate(resolvers):\n",
    "        keys = resolver.keys()\n",
    "\n",
    "        for op in set(q.split()):\n",
    "            matches = [(op, k) for k in keys if re.search(op, k)]\n",
    "\n",
    "            if len(matches) == 1:\n",
    "                op, key = matches[0]\n",
    "                repl.append((idx, op, resolver[key]))\n",
    "\n",
    "            elif len(matches) > 1:\n",
    "                raise KeyError(f\"Ambiguous query operand provided: `{op}`\")\n",
    "\n",
    "    for idx, op, val in repl:\n",
    "        resolvers[idx][op] = val\n",
    "\n",
    "    env = _ensure_scope(level=1, resolvers=resolvers, target=context)\n",
    "    expr = Expr(query, engine=engine, parser=parser, env=env)\n",
    "\n",
    "    def _resolve_operands(operands) -> list:\n",
    "        for op in operands:\n",
    "            # complex query\n",
    "            if op.is_scalar:\n",
    "                continue\n",
    "\n",
    "            if hasattr(op, \"operands\"):\n",
    "                yield from _resolve_operands(op.operands)\n",
    "\n",
    "            yield str(op)\n",
    "\n",
    "    operands = set(_resolve_operands(expr.terms.operands))\n",
    "\n",
    "    for op in operands:\n",
    "        try:\n",
    "            query = query.replace(op, env.resolvers[op].name)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return context.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:07:15.861034Z",
     "start_time": "2019-06-07T13:07:15.837996Z"
    },
    "code_folding": [
     0,
     21
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def _is_valid_group(df: pd.DataFrame, groupby: Union[str, List[str]]):\n",
    "    \"\"\"\"\"\"\n",
    "    is_valid = False\n",
    "    try:\n",
    "        # check that grouping is possible\n",
    "        is_valid = len(df.groupby(groupby).indices) >= 1\n",
    "        if not is_valid:\n",
    "            logger.warning(f\"Column '{groupby!s}' could NOT be used as index group. Dropped.\")\n",
    "    \n",
    "    except TypeError:\n",
    "        logger.warning(f\"Column '{groupby!s}' dtype NOT understood. Dropped\")\n",
    "    \n",
    "    return is_valid\n",
    "\n",
    "\n",
    "def group_inspection_dataframe(\n",
    "    inspection_df: pd.DataFrame,\n",
    "    groupby: Union[str, list, set] = None,\n",
    "    exclude: Union[str, list, set] = None,\n",
    "    as_group: bool = False,\n",
    "    as_index: bool = False,\n",
    "):\n",
    "    \"\"\"\"\"\"\n",
    "    groupby = groupby or []\n",
    "    exclude = exclude or []\n",
    "\n",
    "    if isinstance(groupby, str):\n",
    "        groupby = [groupby]\n",
    "\n",
    "    if isinstance(exclude, str):\n",
    "        exclude = [exclude]\n",
    "\n",
    "    groups = []\n",
    "\n",
    "    for key in groupby:\n",
    "        columns_idx = inspection_df.columns.str.contains(key)\n",
    "        columns = inspection_df.columns[columns_idx]\n",
    "\n",
    "        if not len(columns):\n",
    "            raise KeyError(\n",
    "                f\"Could NOT find suitable column given the keys: `{groupby}`\"\n",
    "            )\n",
    "\n",
    "        groups.extend(columns)\n",
    "\n",
    "    index_groups = []\n",
    "\n",
    "    for col in inspection_df[groups].columns:\n",
    "        # check that the column name is not excluded\n",
    "        if any(re.search(e, col) for e in exclude):\n",
    "            continue\n",
    "\n",
    "        if _is_valid_group(inspection_df, col):\n",
    "            index_groups.append(col)\n",
    "\n",
    "    index_groups = pd.Series(index_groups).unique().tolist()\n",
    "\n",
    "    # construct multi-index if grouping is requested\n",
    "    group = inspection_df.groupby(index_groups)\n",
    "    \n",
    "    if as_group:\n",
    "        return group\n",
    "    \n",
    "    indices = group.indices\n",
    "\n",
    "    levels = []\n",
    "    for level, values in indices.items():\n",
    "        if isinstance(level, tuple):\n",
    "            levels.extend([(*level, v) for v in values])\n",
    "        else:\n",
    "            levels.extend([(level, v) for v in values])\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(levels, names=[*index_groups, None])\n",
    "    \n",
    "    if as_index:\n",
    "        return index\n",
    "\n",
    "    return (\n",
    "        inspection_df.set_index(index).drop(index_groups, axis=1).sort_index(level=-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:07:15.872127Z",
     "start_time": "2019-06-07T13:07:15.864215Z"
    },
    "code_folding": [
     2
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def filter_inspection_dataframe(\n",
    "    inspection_df: pd.DataFrame, like: str = None, regex: str = None, axis: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\"\"\"\n",
    "    if not any([like, regex]):\n",
    "        return inspection_df\n",
    "\n",
    "    filtered_df = inspection_df.filter(like=like, regex=regex, axis=axis)\n",
    "\n",
    "    if not any(filtered_df.columns.str.contains(\"duration\")):\n",
    "        # duration columns must be present\n",
    "        filtered_df = filtered_df.join(inspection_df.filter(like=\"duration\"))\n",
    "\n",
    "    inspection_df = filtered_df\n",
    "\n",
    "    return inspection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:07:15.884158Z",
     "start_time": "2019-06-07T13:07:15.874989Z"
    },
    "code_folding": [
     11
    ],
    "init_cell": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def query_inspection_dataframe(\n",
    "    inspection_df: List[dict],\n",
    "    *,\n",
    "    query: str = None,\n",
    "    groupby: Union[str, list, set] = None,\n",
    "    exclude: Union[str, list, set] = None,\n",
    "    like: str = None,\n",
    "    regex: str = None,\n",
    "    axis: int = None,\n",
    "    sort_index: Union[bool, int, List[int]] = True,\n",
    "    engine: str = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Query inspection DataFrame.\n",
    "    \n",
    "    The order of operations is as follows:\n",
    "    \n",
    "        query resolution -> grouping -> filtering\n",
    "    \n",
    "    :param inspection_df: inspection DataFrame to be filtered as returned by `process_inspection_results`\n",
    "    :param groupby: column or list of columns to group the DataFrame by\n",
    "    :param exclude: patterns that should be excluded from grouping\n",
    "    :param query: pandas query to be evaluated on the filtered DataFrame\n",
    "    :param like, regex, axis: parameters passed to the `pd.DataFrame.filter` function\n",
    "    :param engine: engine to evaluate the query passed to `where` parameter, see `pd.eval` for more information\n",
    "        \n",
    "        The string provided does NOT need to match the whole column name, the function tries to determine\n",
    "        the most suitable column name automatically.\n",
    "        \n",
    "    :param **groupby_kwargs: additional parameters passed to the `pd.DataFrame.groupby` function\n",
    "    \"\"\"\n",
    "    # resolve query\n",
    "    inspection_df = _resolve_query(query=query, context=inspection_df)\n",
    "\n",
    "    if groupby:\n",
    "        inspection_df = group_inspection_dataframe(\n",
    "            inspection_df, groupby=groupby, exclude=exclude\n",
    "        )\n",
    "\n",
    "    # filter\n",
    "    df = filter_inspection_dataframe(inspection_df, like=like, regex=regex, axis=axis)\n",
    "\n",
    "    if sort_index:\n",
    "        if isinstance(sort_index, bool):\n",
    "            levels = np.arange(df.index.nlevels - 1).tolist()\n",
    "        else:\n",
    "            levels = sort_index\n",
    "\n",
    "        return df.sort_index(level=levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:58:31.725348Z",
     "start_time": "2019-06-07T13:58:20.820455Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "df = process_inspection_results(\n",
    "    inspection_results,\n",
    "    exclude=[\"build_log\", \"created\", \"inspection_id\"],\n",
    "    apply=[(\"created|started_at|finished_at\", pd.to_datetime)],\n",
    "    drop=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping based on hardware platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:22:51.599794Z",
     "start_time": "2019-06-07T13:22:51.135470Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = query_inspection_dataframe(df, groupby=\"hwinfo\", exclude=\"node\")\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to group by multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:22:51.917167Z",
     "start_time": "2019-06-07T13:22:51.602813Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = query_inspection_dataframe(df, groupby=[\"ncpus\", \"platform\"], exclude=\"node\")\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally if we are only interested in certain columns, we can filter them as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:22:51.969784Z",
     "start_time": "2019-06-07T13:22:51.922946Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = query_inspection_dataframe(\n",
    "    df, groupby=[\"platform\", \"ncpus\"], like=\"duration\", exclude=\"node\"\n",
    ")\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full-fledged filtering example can also filter based on the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:22:52.321780Z",
     "start_time": "2019-06-07T13:22:52.254829Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = query_inspection_dataframe(\n",
    "    df,\n",
    "    query=\"ncpus == 32\",\n",
    "    groupby=[\"platform\", \"ncpus\"],\n",
    "    like=\"duration\",\n",
    "    exclude=\"node\",\n",
    ")\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping based on exit status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:22:52.743621Z",
     "start_time": "2019-06-07T13:22:52.598136Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = query_inspection_dataframe(\n",
    "    df, like=\"job\", groupby=[\"reason\", \"exit_code\"], exclude=\"build\"\n",
    ")\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of duration dataframe from filtered inspection results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T07:41:45.773324Z",
     "start_time": "2019-05-28T07:41:45.720560Z"
    },
    "scrolled": true
   },
   "source": [
    "Creating the duration dataframe works as expected, by computing statistics for each group separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:22:57.310885Z",
     "start_time": "2019-06-07T13:22:57.050654Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df = query_inspection_dataframe(\n",
    "    df,\n",
    "    groupby=[\"platform\", \"ncpus\"],\n",
    "    like=\"duration\",\n",
    "    query=\"ncpus == 32 | ncpus == 64\",\n",
    "    exclude=[\"node\", \"platform__version\"],\n",
    ")\n",
    "\n",
    "df_duration = create_duration_dataframe(filtered_df)\n",
    "df_duration.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing grouped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:07:23.596298Z",
     "start_time": "2019-06-07T13:07:23.581955Z"
    },
    "code_folding": [
     4,
     34,
     56
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_column_group(\n",
    "    df: pd.DataFrame,\n",
    "    columns: Union[List[Union[str, int]], pd.Index] = None,\n",
    "    label: str = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\"\"\"\n",
    "    columns = columns or df.columns\n",
    "\n",
    "    if all(isinstance(c, int) for c in columns):\n",
    "        columns = [df.columns[i] for i in columns]\n",
    "\n",
    "    if not label:\n",
    "        cols = [col.split(\"_\") for col in columns]\n",
    "\n",
    "        common_words = set(functools.reduce(np.intersect1d, cols))\n",
    "        if common_words:\n",
    "            label = \"_\".join(w for w in cols[0] if w in common_words).strip(\"_\")\n",
    "            \n",
    "            if len(label) <= 0:\n",
    "                label = str(tuple(columns))\n",
    "        else:\n",
    "            label = str(tuple(columns))\n",
    "\n",
    "    Group = namedtuple(\"Group\", columns)\n",
    "\n",
    "    groups = []\n",
    "    for i, row in df[columns].iterrows():\n",
    "        groups.append(Group(*row))\n",
    "\n",
    "    return pd.Series(groups, name=label)\n",
    "\n",
    "\n",
    "def get_index_group(\n",
    "    df: pd.DataFrame, names: List[Union[str, int]] = None, label: str = None\n",
    ") -> pd.Series:\n",
    "    \"\"\"\"\"\"\n",
    "    names = names or list(filter(bool, df.index.names[:-1]))\n",
    "\n",
    "    if all(isinstance(n, int) for n in names):\n",
    "        names = [df.index.names[i] for i in names]\n",
    "\n",
    "    index = df.index.to_frame(index=False)\n",
    "    group = get_column_group(index[names])\n",
    "\n",
    "    index = index.drop(columns=names)\n",
    "    group_indices = pd.DataFrame(group).join(index).values.tolist()\n",
    "\n",
    "    group_index = pd.MultiIndex.from_tuples(\n",
    "        group_indices, names=[group.name, *index.columns[:-1], None]\n",
    "    )\n",
    "\n",
    "    return group_index\n",
    "\n",
    "\n",
    "def set_index_group(\n",
    "    df: pd.DataFrame, names: List[Union[str, int]] = None, label: str = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\"\"\"\n",
    "    group_index = get_index_group(df, names, label)\n",
    "\n",
    "    return df.set_index(group_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T19:22:55.915361Z",
     "start_time": "2019-06-07T19:22:55.882310Z"
    },
    "code_folding": [],
    "init_cell": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_subplots(\n",
    "    data: pd.DataFrame, columns: List[str] = None, *, kind: str = \"box\", **kwargs\n",
    "):\n",
    "    \"\"\"\"\"\"\n",
    "    if kind not in (\"box\", \"histogram\", \"scatter\", \"scatter_with_bounds\"):\n",
    "        raise ValueError(f\"Can NOT handle plot of kind: {kind}.\")\n",
    "\n",
    "    index = data.index.droplevel(-1).unique()\n",
    "\n",
    "    if len(index.names) > 2:\n",
    "        logger.warning(\n",
    "            f\"Can only handle hierarchical index of depth <= 2, got {len(index.names)}. Grouping index.\"\n",
    "        )\n",
    "\n",
    "        return make_subplots(\n",
    "            set_index_group(data, range(index.nlevels - 1)),\n",
    "            columns,\n",
    "            kind=kind,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    grid = ff.create_facet_grid(\n",
    "        data.reset_index(),\n",
    "        facet_row=index.names[1] if index.nlevels > 1 else None,\n",
    "        facet_col=index.names[0],\n",
    "        trace_type=\"box\",  # box does not need data specification\n",
    "        ggplot2=True,\n",
    "    )\n",
    "\n",
    "    shape = np.shape(grid._grid_ref)[:-1]\n",
    "\n",
    "    sub_plots = tools.make_subplots(\n",
    "        rows=shape[0],\n",
    "        cols=shape[1],\n",
    "        shared_yaxes=kwargs.pop(\"shared_yaxes\", True),\n",
    "        shared_xaxes=kwargs.pop(\"shared_xaxes\", False),\n",
    "        print_grid=kwargs.pop(\"print_grid\", False),\n",
    "    )\n",
    "\n",
    "    if isinstance(index, pd.MultiIndex):\n",
    "        index_grid = zip(*index.labels)\n",
    "    else:\n",
    "        index_grid = iter(np.transpose([\n",
    "            np.tile(np.arange(shape[1]), shape[0]), np.repeat(np.arange(shape[0]), shape[1])\n",
    "        ]))\n",
    "        \n",
    "    for idx, grp in data.groupby(level=np.arange(index.nlevels).tolist()):\n",
    "        if not isinstance(columns, str) and kind == \"scatter_with_bounds\":\n",
    "            if columns is None:\n",
    "                raise ValueError(\n",
    "                    \"`scatter_with_bounds` requires `col` argument, not provided.\"\n",
    "                )\n",
    "            try:\n",
    "                columns, = columns\n",
    "            except ValueError:\n",
    "                raise ValueError(\n",
    "                    \"`scatter_with_bounds` does not allow for multiple columns.\"\n",
    "                )\n",
    "\n",
    "        fig = eval(f\"create_duration_{kind}(grp, columns, **kwargs)\")\n",
    "\n",
    "        col, row = map(int, next(index_grid))  # col-first plotting\n",
    "        for trace in fig.data:\n",
    "            sub_plots.append_trace(trace, row + 1, col + 1)\n",
    "\n",
    "    layout = sub_plots.layout\n",
    "    layout.update(\n",
    "        title=kwargs.get(\"title\", fig.layout.title),\n",
    "        shapes=grid.layout.shapes,\n",
    "        annotations=grid.layout.annotations,\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    x_dom_vals = [k for k in layout.to_plotly_json().keys() if \"xaxis\" in k]\n",
    "    y_dom_vals = [k for k in layout.to_plotly_json().keys() if \"yaxis\" in k]\n",
    "\n",
    "    layout_shapes = pd.DataFrame(layout.to_plotly_json()[\"shapes\"]).sort_values(\n",
    "        [\"x0\", \"y0\"]\n",
    "    )\n",
    "\n",
    "    h_shapes = layout_shapes[~layout_shapes.x0.duplicated(keep=False)]\n",
    "    v_shapes = layout_shapes[~layout_shapes.y0.duplicated(keep=False)]\n",
    "    \n",
    "    # handle single-columns\n",
    "    h_shapes = h_shapes.query(\"y1 - y0 != 1\")\n",
    "    v_shapes = v_shapes.query(\"x1 - x0 != 1\")\n",
    "\n",
    "    # update axis domains and layout\n",
    "    for idx, x_axis in enumerate(x_dom_vals):\n",
    "        x0, x1 = h_shapes.iloc[idx % shape[1]][[\"x0\", \"x1\"]]\n",
    "\n",
    "        layout[x_axis].domain = (x0 + 0.03, x1 - 0.03)\n",
    "        layout[x_axis].update(showticklabels=False, zeroline=False)\n",
    "\n",
    "    for idx, y_axis in enumerate(y_dom_vals):\n",
    "        y0, y1 = v_shapes.iloc[idx % shape[0]][[\"y0\", \"y1\"]]\n",
    "\n",
    "        layout[y_axis].domain = (y0 + 0.03, y1 - 0.03)\n",
    "        layout[y_axis].update(zeroline=False)\n",
    "        \n",
    "    # correct annotation to match the relevant group and width\n",
    "    annot_df = pd.DataFrame(layout.to_plotly_json()['annotations']).sort_values(['x', 'y'])\n",
    "    annot_df = annot_df[annot_df.text.str.len() > 0]\n",
    "    \n",
    "    aw = min(  # annotation width magic\n",
    "        int(max(60 / shape[1] - ( 2 * shape[1]), 6)),\n",
    "        int(max(30 / shape[0] - ( 2 * shape[0]), 6))\n",
    "    )\n",
    "    \n",
    "    for i, annot_idx in enumerate(annot_df.index):\n",
    "        annot = layout.annotations[annot_idx]\n",
    "        \n",
    "        index_label: Union[str, Any] = annot[\"text\"]\n",
    "        if isinstance(index, pd.MultiIndex):\n",
    "            index_axis = i >= shape[1]\n",
    "            if shape[0] == 1:\n",
    "                pass  # no worries, the order and label are aight\n",
    "            elif shape[1] == 1:\n",
    "                index_label = index.levels[index_axis][max(0, i - 1)]\n",
    "            else:\n",
    "                index_label = index.levels[index_axis][i % shape[1]]\n",
    "        \n",
    "        text: str = str(index_label)\n",
    "        \n",
    "        annot[\"text\"] = re.sub(r\"^(.{%d}).*(.{%d})$\" % (aw, aw), \"\\g<1>...\\g<2>\", text)\n",
    "        annot[\"hovertext\"] = \"<br>\".join(pformat(index_label).split(\"\\n\"))\n",
    "        \n",
    "    # add axis titles as plot annotations\n",
    "    layout.annotations = (\n",
    "        *layout.annotations,\n",
    "        {\n",
    "            \"x\": 0.5,\n",
    "            \"y\": -0.05,\n",
    "            \"xref\": \"paper\",\n",
    "            \"yref\": \"paper\",\n",
    "            \"text\": fig.layout.xaxis[\"title\"][\"text\"],\n",
    "            \"showarrow\": False\n",
    "        },\n",
    "        {\n",
    "            \"x\": -0.05,\n",
    "            \"y\": 0.5,\n",
    "            \"xref\": \"paper\",\n",
    "            \"yref\": \"paper\",\n",
    "            \"text\": fig.layout.yaxis[\"title\"][\"text\"],\n",
    "            \"textangle\": -90,\n",
    "            \"showarrow\": False\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # custom user layout updates\n",
    "    user_layout = kwargs.pop(\"layout\", None)\n",
    "    if user_layout:\n",
    "        layout.update(user_layout)\n",
    "\n",
    "    return sub_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T19:31:45.956835Z",
     "start_time": "2019-06-07T19:31:45.094405Z"
    }
   },
   "outputs": [],
   "source": [
    "d = query_inspection_dataframe(df, groupby=[\"platform\", \"ncpus\"], exclude=\"node\")\n",
    "d = create_duration_dataframe(d)\n",
    "\n",
    "fig = make_subplots(d, kind=\"histogram\", columns=[\"job_duration\"])\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping based on software stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:54:11.283059Z",
     "start_time": "2019-06-07T13:54:10.865249Z"
    }
   },
   "outputs": [],
   "source": [
    "d = query_inspection_dataframe(df, groupby=[\"requirements_locked__default\"])\n",
    "d.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping based on OS system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:25:14.383374Z",
     "start_time": "2019-06-07T13:25:14.347778Z"
    }
   },
   "outputs": [],
   "source": [
    "d = query_inspection_dataframe(df, groupby=[\"base\"], like=\"duration\", exclude=\"node\")\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:07:23.636608Z",
     "start_time": "2019-06-07T13:07:23.629974Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def show_categories(inspection_df):\n",
    "    \"\"\"List categories and if requested plot them\"\"\"\n",
    "    index = inspection_df.index.droplevel(-1).unique()\n",
    "    \n",
    "    for n, idx in enumerate(index.values):\n",
    "        print(\"\\nCategory {}/{}\".format(n + 1, len(index)))\n",
    "        if len(index.names) > 1:\n",
    "            for name, ind in zip(index.names, idx):\n",
    "                print(f\"{name} :\",ind)\n",
    "        else:\n",
    "            print(f\"{index.names[0]} :\",idx)\n",
    "\n",
    "        frame = inspection_df.loc[idx]\n",
    "        print(\"Number of rows (jobs) is:\", frame.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:54:00.167000Z",
     "start_time": "2019-06-07T13:53:59.983448Z"
    }
   },
   "outputs": [],
   "source": [
    "d = query_inspection_dataframe(df, groupby=\"specification__python__requirements_locked__default\", exclude=\"node\")\n",
    "# Display the categories identified\n",
    "\n",
    "show_categories(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T13:54:00.208225Z",
     "start_time": "2019-06-07T13:54:00.169959Z"
    }
   },
   "outputs": [],
   "source": [
    "dn = query_inspection_dataframe(df, groupby=\"job_log__hwinfo__cpu__ncpus\", exclude=\"node\")\n",
    "# Display the categories identified\n",
    "\n",
    "show_categories(dn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T14:32:53.044747Z",
     "start_time": "2019-06-07T14:32:52.979942Z"
    }
   },
   "outputs": [],
   "source": [
    "dn = query_inspection_dataframe(df, groupby=\"platform\", exclude=\"node\")\n",
    "# Display the categories identified\n",
    "\n",
    "show_categories(dn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T14:30:34.565519Z",
     "start_time": "2019-06-07T14:30:34.497721Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dn = query_inspection_dataframe(df, groupby=\"Athlon\", exclude=\"node\")\n",
    "# Display the categories identified\n",
    "\n",
    "show_categories(dn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T14:25:51.526703Z",
     "start_time": "2019-06-07T14:25:51.490725Z"
    }
   },
   "outputs": [],
   "source": [
    "dn = query_inspection_dataframe(df, groupby=\"platform__release\", exclude=\"node\")\n",
    "# Display the categories identified\n",
    "\n",
    "show_categories(dn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "finalized": {
   "timestamp": 1559895494427,
   "trusted": true
  },
  "hide_input": false,
  "jupytext": {
   "formats": "ipynb,py:hydrogen"
  },
  "kernelspec": {
   "display_name": "thoth-notebooks",
   "language": "python",
   "name": "thoth-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
